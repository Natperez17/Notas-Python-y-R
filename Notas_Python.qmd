---
project:
  type: book

book:
  title: "Notas Python"
  chapters:
    - index.qmd
    - variables-indicadoras-o-dummies.qmd
    # otros capítulos...
format:
  html:
    theme: cosmo
    toc: true
    number-sections: true
    code-fold: true
    search: true
    jupyter: python3
---

```{python}
# Cargar librerías
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

# Introducción

Estas notas fueron realizadas con la finalidad de ser un recurso fundamental para el análisis de datos. Una guía que permita agilizar y tomar decisiones rápidas en cuanto a los procedimientos ideales para el procesamiento, análisis, inferencia y viasualización de datos en R.

Nota: Todos los recursos empleados en este documento son extraídos de notas de clases, páginas web y propios.

# Análisis Exploratorio y Descriptivos

Uno de los pasos más importantes al realizar un análisis de datos, es explorar y describir los datos con los que se cuentan. En esta sección se revisará temas como: Datos faltantes, reemplazar valores, visualización de datos e incluso prubas no parámetricas.

## Datos de clientes

La base de datos data_clientes contiene información de los clientes de una Cooperativa, se cuentan con 4117 registro y 11 columnas con variables que incluyen un identificador único, así como características sociodemográficas como la edad, el género y el estado civil. También recoge variables económicas como los ingresos anuales, el número de hijos, el número de tarjetas, la modalidad de pago, la tenencia de hipoteca y el número de préstamos activos. Finalmente, incluye la variable RIESGO, que clasifica a los clientes según su perfil crediticio, y puede ser utilizada como variable objetivo en modelos de predicción o segmentación.

 
```{python}

# Cargar base de datos
data_clientes = pd.read_csv("C:/Users/natal/Escritorio/Semestres/Semestre 10/Mineria/Data Clientes Cooperativa.txt", sep="\t", na_values=["NA", " ", ""])

# Revisar los primeros 6 registros de la base de datos
data_clientes.head()

# Estructura e información de la base de datos
data_clientes.info()

# Estadísticas descriptivas de las variables de la base de datos
data_clientes.describe()

data_clientes['ID'].unique() # Devuelve los valores únicos de la variable ID
```

## Descripción de datos

### Descripción de datos cuantitativos

Este código permite obtener información detallada de las estadísticas descriptivas para las variables numéricas, como el n, desviación estándar, IQR, etc. Esta información es mucho más completa que el describe().

```{python}
variables_Num = ['EDAD', 'INGRESOS', 'NUM_HIJOS', 'NUM_TARJETAS', 'PRESTAMOS']

# Diccionario para almacenar los resultados
resumen = {}

# Recorremos cada variable y calculamos las estadísticas
for var in variables_Num:
    x = data_clientes[var].dropna()
    resumen[var] = {
        'n': len(x),
        'media': round(np.mean(x), 2),
        'Desv.Est': round(np.std(x, ddof=1), 2),
        'mediana': round(np.median(x), 2),
        'IQR': round(np.percentile(x, 75) - np.percentile(x, 25), 2),
        'Percentil25': round(np.percentile(x, 25), 2),
        'Percentil75': round(np.percentile(x, 75), 2),
        'Percentil95': round(np.percentile(x, 95), 2)
    }

# Convertimos el diccionario a un DataFrame y lo transponemos
tabla_descrip = pd.DataFrame(resumen)

# Mostramos solo la tabla final completa
print(tabla_descrip)
```


### Descripción de datos cualitativos

Este código genera las frecuencias absolutas y relativas para todas las variables cualitativas a tráves de un for

```{python}

variables_Cat = ['GENERO', 'ESTADO_CIVIL', 'MODALIDAD_PAGO', 'HIPOTECA', 'RIESGO']

FrecA_tablas_Cat = {
  var: data_clientes[var].value_counts(dropna=False).reset_index().rename(columns={
    'index': var,
    var: 'Frecuencia'
  }) for var in variables_Cat
}

FrecA_tablas_Cat
```

```{python}
FrecR_tablas_Cat = {
  var: data_clientes[var].value_counts(normalize=True,dropna=False).reset_index().rename(columns={
    'index': var,
    var: 'Frecuencia'
  }) for var in variables_Cat
}

FrecR_tablas_Cat
```

## Datos faltantes

La figura es muy útil para visualizar los datos faltantes por variable. En este caso hay muy pocos por lo que no se logran ver.

```{python}
import missingno as msno
msno.matrix(data_clientes)
plt.title("Datos faltantes ")
plt.show()
```

```{python}
data_clientes.isna().sum()
```

## Modificar categorías de las variables

```{python}
data_clientes['GENERO'] = data_clientes['GENERO'].replace({'f': 'Femenino', 'm': 'Masculino'})

data_clientes['ESTADO_CIVIL'] = data_clientes['ESTADO_CIVIL'].replace({'divsepwid': 'Viudo/Divorciado',
'married': 'Casado',
'single':'Soltero'})

data_clientes['MODALIDAD_PAGO'] = data_clientes['MODALIDAD_PAGO'].replace({'monthly': 'Mensual',
'weekly': 'Semanal'})

data_clientes['HIPOTECA'] = data_clientes['HIPOTECA'].replace({'n': 'NO',
'y': 'SI'})

data_clientes['RIESGO'] = data_clientes['RIESGO'].replace({'F':'Cumplimiento', 'V': 'Impago'})

data_clientes.head()

```


## Pruebas No paramétricas

La pruebas parámétricas son esenciales cuando se quiere realizar análisis bivariados sin asumir una distribución específica de los datos queu generalemnte es la normalidad. Existen varias pruebas para realizar inferencias respecto a las relaciones, sin embargo hay ciertas condiciones que se deben tener en cuenta como lo muestra la siguiente tabla:


| Tipo de Variables Comparadas           | Condición                   | Prueba Estadística                  |
| -------------------------------------- | --------------------------- | ----------------------------------- |
| **2 variables numéricas**              | Distribución normal         | **Correlación de Pearson**          |
|                                        | No normal                   | **Correlación de Spearman**         |
| **1 variable numérica + 1 categórica** | Distribución normal         | 2 categorías: **t de Student**      |
|                                        |                             | >2 categorías: **ANOVA**            |
|                                        | No normal                   | 2 categorías: **U de Mann-Whitney** |
|                                        |                             | >2 categorías: **Kruskal-Wallis**   |
| **2 variables categóricas**            | Tabla 2x2                   | **Test de Fisher**                  |
|                                        | Tabla r x s (>2 categorías) | **Chi-cuadrado (χ²)**               |

### Evaluar normalidad

La prueba de Shapiro-Wilk se utiliza para evualar la normalidad de una variable, el comando es shapiro.test().

(H₀): Los datos provienen de una distribución normal.

(H₁): Los datos no provienen de una distribución normal.

**Interpretación del p-valor**:

Si **p > 0.05**: no se rechaza H₀ → los datos son compatibles con una distribución normal.

Si **p ≤ 0.05**: se rechaza H₀ → los datos no siguen una distribución normal.

```{python}
from scipy.stats import shapiro

Norm_tablas_Num = {var: shapiro(data_clientes[var].dropna()) for var in variables_Num}

for var, resultado in Norm_tablas_Num.items():
  print(f"{var}: W={resultado.statistic:4f}, p-value={resultado.pvalue:.4f}")
```

### Correlación

Es una medida estadística que indica la fuerza y dirección de una relación lineal entre dos variables cuantitativas.El resultado se encuentra en un rango entre [-1,1].
**-1** : Correlación negativa perfecta (una variable sube y otra baja)
**0**: No hay correlación
**1**: Correlación positiva perfecta (ambas variables aumentan o disminuyen juntas)

```{python}
data_clientes['INGRESOS'].corr(data_clientes['EDAD'], method='spearman')

# Crear la matriz de correlación con método Spearman
matriz_cor = data_clientes[variables_Num].corr(method='spearman')

mask = np.triu(np.ones_like(matriz_cor, dtype=bool))

# Crear mapa de calor con los coeficientes
sns.heatmap(matriz_cor,
            mask=mask,
            annot = True,     # Mostrar los valores numéricos
            fmt='.2f',   # Formato de decimales
            cmap='coolwarm', # Paleta de colores
            square=True,
            linewidths=0.5,
            cbar_kws={'shrink': .75})

plt.title('Matriz de correlación (Spearman)')
plt.show()
```

### U de Mann-Whitney

Esta prueba evalúa si hay diferencia entre las medianas de los grupos.El comando difiere al nombre ya que es wilcox.test().

(H₀): La mediana de los grupos son iguales.

(H₁): La mediana de los grupos son diferentes.

Si **p ≤ 0.05**: se rechaza H₀ → Hay diferencia en las medianas de los grupos.

```{python}
from scipy.stats import mannwhitneyu

# Separar los grupos según el género
ingresos_f = data_clientes[data_clientes['GENERO'] == 'Femenino']['INGRESOS'].dropna() 

ingresos_m = data_clientes[data_clientes['GENERO'] == 'Masculino']['INGRESOS'].dropna()

# Prueba de Mann-Whitney (equivalente a Wilcoxon para muestras indepepndientes)

stat, p = mannwhitneyu(ingresos_f, ingresos_m, alternative = 'two-sided')

# Mostrar resultados
print(f'Estadístico U={stat:4f}, p.value = {p:4f}')
```

### Otros

Para T-Student y ANOVA se evalúa la diferencia entre las medias de los grupos. Parael test de Kruskall Wallis la diferencia entre las medianas de los grupos. Para el Test de fisher y Chi-cuadrado se evalúa la indepencia. Siempre se busca rechazar la Hiótesis nula, es decir, **p ≤ 0.05**.

**T-Student**

```{python}
from scipy.stats import ttest_ind

grupo1 = data_clientes[data_clientes['GENERO'] =='Femenino']['INGRESOS'].dropna()

grupo2 = data_clientes[data_clientes['GENERO'] =='Masculino']['INGRESOS'].dropna()

t_stat, p_val = ttest_ind(grupo1, grupo2, equal_var=True)
```


**ANOVA**

```{python}
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.anova import anova_lm

modelo =smf.ols('INGRESOS ~ ESTADO_CIVIL', data = data_clientes).fit()
anova = anova_lm(modelo)
print(anova)

# Test de Barlett (Homogeneidad de varianzas)

#from scipy.stats import barlett

grupo1 = data_clientes[data_clientes["ESTADO_CIVIL"] == "Soltero"]["INGRESOS"].dropna()

grupo2 = data_clientes[data_clientes["ESTADO_CIVIL"] == "Casado"]["INGRESOS"].dropna()

grupo3 = data_clientes[data_clientes["ESTADO_CIVIL"] == "Viudo/Divorciado"]["INGRESOS"].dropna()

#barlett_stat, p_barlett = barlett(grupo1, grupo2, grupo3)

```

**Kruskal Wallis** 

```{python}

from scipy.stats import kruskal
kruskal_stats, p_kruskal = kruskal(grupo1, grupo2, grupo3)

```

**Test de fisher**

```{python}
from scipy.stats import fisher_exact

tabla =pd.crosstab(data_clientes['HIPOTECA'], data_clientes['RIESGO'])

oddsratio, p_fisher = fisher_exact(tabla)
```

**Chi-cuadrado**

```{python}
from scipy.stats import chi2_contingency

tabla = pd.crosstab(data_clientes['GENERO'], data_clientes['RIESGO'])
chi2, p_chi, dof, expected = chi2_contingency(tabla)
```

## Visualización de datos

### Gráfico de barras
```{python}

# Colores personalizados
colores = ["#1C86EE", "#458B00", "#EE1289"]

# Crear grafíco
plt.figure(figsize=(8,6))
grafico = sns.countplot(
  data=data_clientes,
  x = 'ESTADO_CIVIL',
  hue= 'ESTADO_CIVIL',
  palette=colores
)

for p in grafico.patches:
  grafico.text(p.get_x() + p.get_width()/2,
  p.get_height(),
  int(p.get_height()),
  ha = 'center', va='bottom')

plt.title('Conteo de la variable Estado Civil por categoría')
plt.xlabel('Estado civil')
plt.ylabel('Frecuencia Absoluta')

plt.show()
```

### Boxplot

```{python}
plt.figure(figsize=(8,6))

sns.boxplot(
  data=data_clientes,
  x = 'GENERO',
  y = 'INGRESOS',
  color = '#1C86EE',)

plt.title('Gráfico de cajas: Ingresos por Género')
plt.xlabel('Género')
plt.ylabel('Ingresos')

plt.show()
```

### Gráfico de Violín

```{python}
sns.violinplot(x='RIESGO', y='PRESTAMOS', data=data_clientes)

plt.title('Riesgo de incumplimiento por Número de Préstamos')
plt.ylabel('Riesgo de incumplimiento')
plt.show()
```

### Gráfico de Dispersión

```{python}

sns.scatterplot(data = data_clientes, x= 'EDAD', y= 'INGRESOS')

plt.title('Gráfico de dispersión Edad vs Ingresos')
plt.show()
```


# Modelos

## Modelo Lineal Simple

```{python}
# Importar librerías
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

```


```{python}
data_clientes['RIESGO_bin'] = data_clientes['RIESGO'].map({'Cumplimiento': 0, 'Impago':1})
data_clientes['RIESGO_bin'].unique()
```


```{python}
import statsmodels.api as sm

X = sm.add_constant(data_clientes[['RIESGO_bin']], prepend=True)
y = data_clientes['INGRESOS']

modelo = sm.OLS(endog=y, exog=X)
resultado = modelo.fit()

print(resultado.summary())

```


# Modelo Lineal Múltiple

```{python}

X = data_clientes.drop(columns=['ID', 'INGRESOS', 'RIESGO_bin'])

X = pd.get_dummies(X, drop_first=True)
X = sm.add_constant(X, prepend=True)
y = data_clientes['INGRESOS']

X = X.astype(float)
y = y.astype(float)

modelo = sm.OLS(endog=y, exog=X)
modelo = modelo.fit()

print(modelo.summary())

```


## Modelo Logístico Simple
```{python}
# Cargar librerías

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import statsmodels.formula.api as smf
from statsmodels.stats.weightstats import ttest_ind

# Configurar warnings
import warnings
warnings.filterwarnings('ignore')
```


```{python}
X = sm.add_constant(data_clientes[['EDAD']], prepend=True)
y = data_clientes['RIESGO_bin']
modelo_LSimple = sm.Logit(endog=y, exog=X)
modelo_LSimple = modelo_LSimple.fit()
print(modelo_LSimple.summary())

```

```{python}

print("AIC:", modelo_LSimple.aic)
print("BIC:", modelo_LSimple.bic)

```


```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score

# Se hace un nuevo modelo porque el AUC funciona con LogisticRegression en ves de sm.Logit
modelo_LSimple2 = LogisticRegression()
modelo_LSimple2.fit(X, y)

y_prob = modelo_LSimple2.predict_proba(X)[:, 1]
auc = roc_auc_score(y, y_prob)
print("AUC:", auc)

```


## Validación Cruzada Modelo Logístico
```{python}

X = data_clientes.drop(columns=['ID', 'RIESGO', 'RIESGO_bin'])
X = pd.get_dummies(X, drop_first=True)
X = sm.add_constant(X, prepend=True)

y = data_clientes['RIESGO_bin']

# Unir X e y para eliminar filas con NaN en cualquiera
df_modelo = pd.concat([X, y], axis=1).dropna()

X = df_modelo.drop(columns=['RIESGO_bin']).astype(float)
y = df_modelo['RIESGO_bin'].astype(float)

X_train, X_test, y_train, y_test = train_test_split(X, # Convierte X en un array de Numpy (haciendo tantas filas como sea necesario)
y, # Convierte Y en un array de Numpy (haciendo tantas filas como sea necesario)
train_size = 0.8, # Se usarán el 80% de los registros para entrenar el modelo
random_state =1234, # Fijar semilla (Creo que no hace falta)
shuffle = True) # Mezcla aleatoriamente todos los datos antes de dividirlos (Creo que tampoco hace falta)

```

```{python}
X_train = sm.add_constant(X_train, prepend=True) # Agrega una columna al principio para el intercepto
modelo_LMultiple = sm.Logit(endog=y_train, exog=X_train) # Inicializa un modelo de regresión logística
modelo_LMultiple = modelo_LMultiple.fit() # Ajustar modelo
print(modelo_LMultiple.summary()) # Imprimir el resumen del modelo

```

```{python}

print("AIC:", modelo_LMultiple.aic)
print("BIC:", modelo_LMultiple.bic)

```

```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score

# Se hace un nuevo modelo porque el AUC funciona con LogisticRegression en ves de sm.Logit
modelo_LMultiple2 = LogisticRegression()
modelo_LMultiple2.fit(X_train, y_train)

y_prob = modelo_LMultiple2.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_prob)
print("AUC:", auc)

```

```{python}
# Predicción de probabilidades
predicciones = modelo_LMultiple.predict(exog= X_train)
predicciones[0:15]
```

```{python}

# Clasificación predicha
clasificacion = np.where(predicciones<0.5, 0, 1)
pd.Series(clasificacion).value_counts()
```

## Ridge
La regularización Ridge penaliza la suma de los coeficientes elevados al cuadrado. A esta penalización se le conoce como l2 y tiene el efecto de reducir de forma proporcional el valor de todos los coeficientes del modelo pero sin que estos lleguen a cero. El grado de penalización está controlado por el hiperparámetro /lambda. Cuando /lambda=0, la penalización es nula y el resultado es equivalente al de un modelo lineal por mínimos cuadrados ordinarios (OLS). A medida que /lambda aumenta, mayor es la penalización y menor el valor de los predictores. 

El método ridge es capaz de reducir la varianza sin apenas aumentar el sesgo  estimador (valor esperado de un estimador y el valor verdadero del parámetro), consiguiendo así un menor error total. 

La desventaja del método ridge es que, el modelo final, incluye todos los predictores. Esto es así porque, si bien la pemalización fuerza a que los coefientes tiendan a cero, nunca llegan a ser exactamente cero (solo si /lambda=0).

```{python}

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import RidgeCV
from sklearn.linear_model import LassoCV
from sklearn.linear_model import ElasticNetCV
```

```{python}

modelo_Ridge = make_pipeline(
    StandardScaler(),
    RidgeCV(alphas=np.logspace(-4, 4, 200), cv=5)
)

# Ajustar el modelo
modelo_Ridge = modelo_Ridge.fit(X, y)
```

```{python}

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler

# Escalamos los datos (como haría StandardScaler en el pipeline)
scaler = StandardScaler()
X_esc = scaler.fit_transform(X)

# Definimos los valores de alpha
alphas = np.logspace(-10, 2, 200)

# Calculamos los coeficientes para cada alpha
coefs = []

# fit_intercept es False porque previamente habíamos escalado los datos
for alpha in alphas:
    modelo_temp = Ridge(alpha=alpha, fit_intercept=False)
    modelo_temp.fit(X_esc, y)
    coefs.append(modelo_temp.coef_.flatten())

# Graficamos
fig, ax = plt.subplots(figsize=(7, 4))
ax.plot(alphas, coefs)
ax.set_xscale('log')
ax.set_ylim([-15, None])
ax.set_xlabel('alpha')
ax.set_ylabel('coeficientes')
ax.set_title('Coeficientes del modelo Ridge en función de la regularización');

```

```{python}

# Definir la grilla de alphas
alphas = np.logspace(-10, 2, 200)

# Ajustar el modelo con CV (por defecto usa leave-one-out si no se especifica)
modelo_ridgecv = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error', cv=5)
modelo_ridgecv.fit(X_esc, y)

# Mostrar el mejor alpha
mejor_alpha = modelo_ridgecv.alpha_
print(f"El mejor alpha según CV es: {mejor_alpha:.6f}")
```


```{python}

from sklearn.linear_model import LinearRegression
modelo_lr = LinearRegression()
modelo_lr.fit(X, y)
mse_lr = mean_squared_error(y, modelo_lr.predict(X))

```

```{python}

from sklearn.linear_model import Ridge
modelo_ridge = Ridge(alpha=1e-3)  # prueba con uno pequeño, como 0.001
modelo_ridge.fit(X, y)
mse_ridge = mean_squared_error(y, modelo_ridge.predict(X))


```

```{python}

print(f"MSE sin regularización: {mse_lr:.4f}")
print(f"MSE con Ridge(alpha=1e-3): {mse_ridge:.4f}")

```


Los anteriores resultados sugieren que el modelo no se beneficia del la regularización Ridge

## Lasso
La regularización Lasso penaliza la suma del valor absoluto de los coeficientes de regresión. A esta penalización sse le como l1 y tiene el efecto de forzar a que los coeficientes de los predictores tiendan a cero. Dado que un predictor con coeficiente de regresión cero no influye en el modelo, lasso consigue excluir los predictores menos relevantes. Al igual que en ridge, el grado de penalización está controlado por el hiperparámetro 
/lambda. Cuando /lambda=0, el resultado es quivalente al de un modelo lineal por mínimos cuadrados ordinarios. A medida que /lambda aumenta, mayor es la penalización y más predictores quedan exluidos.



### Comparación Ridge y Lasso
La principal diferencia práctica entre lasso y ridge es que el primero consigue que algunos coeficientes sean exactamente cero (0), por lo que realiza selección de predictoras, mientras que el segundo no llega a excluir ninguno. Esto supone una ventaja notable de lasso en escenarios donde no todos los predictoras son importantes para el modelo y se desea que los menos influyentes queden excluidos.

## Elastin net
Elastic net incluye una regularización que combina la penalización l1 y l2. El grado en que influye cada una de las penalizaciones está controlado por el hiperparámetro. Cuando el hiperparámetro es 0 se aplica ridge y cuando el hiperparámetro es 1 se aplica lasso. La combinación de ambas penalizaciones suele dar lugar a buenos reultados.


